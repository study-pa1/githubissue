{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import logging\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "pd.set_option('display.max_colwidth', 500)\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 0\r\n",
      "drwxr-xr-x 2 root root  6 Aug 21  2018 .\r\n",
      "drwxr-xr-x 1 root root 40 Feb  1 09:26 ..\r\n"
     ]
    }
   ],
   "source": [
    "# Ensure that the github-issues-data volume is mounted in /mnt\n",
    "!ls -la /mnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: DATA_DIR=/data\n"
     ]
    }
   ],
   "source": [
    "# Set path for data dir\n",
    "%env DATA_DIR=/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-02-01 10:41:33--  https://storage.googleapis.com/kubeflow-examples/github-issue-summarization-data/github-issues.zip\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 172.253.63.128, 172.253.115.128, 172.217.2.112, ...\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|172.253.63.128|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1027424178 (980M) [application/zip]\n",
      "Saving to: ‘/data/github-issues.zip’\n",
      "\n",
      "github-issues.zip   100%[===================>] 979.83M  6.53MB/s    in 2m 31s  \n",
      "\n",
      "2021-02-01 10:44:05 (6.48 MB/s) - ‘/data/github-issues.zip’ saved [1027424178/1027424178]\n",
      "\n",
      "Archive:  /data/github-issues.zip\n",
      "  inflating: /data/github_issues.csv  \n"
     ]
    }
   ],
   "source": [
    "# Download the github-issues.zip training data to /mnt/github-issues-data\n",
    "!wget --directory-prefix=${DATA_DIR} https://storage.googleapis.com/kubeflow-examples/github-issue-summarization-data/github-issues.zip\n",
    "\n",
    "# Unzip the file into /mnt/github-issues-data directory\n",
    "!unzip ${DATA_DIR}/github-issues.zip -d ${DATA_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a symlink from <current_directory>/github-issues-data to /mnt/github-issues-data\n",
    "!ln -sf ${DATA_DIR} github-issues-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 jovyan users 2.7G Jan 17  2018 github-issues-data/github_issues.csv\r\n"
     ]
    }
   ],
   "source": [
    "# Make sure that the github-issues-data symlink is created\n",
    "!ls -lh github-issues-data/github_issues.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into train and test set and preview data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 1,800 rows 3 columns\n",
      "Test: 200 rows 3 columns\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>issue_url</th>\n",
       "      <th>issue_title</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>618568</th>\n",
       "      <td>\"https://github.com/topcoder-platform/community-app/issues/18\"</td>\n",
       "      <td>font loading issue in dev</td>\n",
       "      <td>loading fonts as separate files in dev leads to problems with hmr. i've changed it to load fonts as data urls within js bundle. this solves the problem with hmr, but delays proper rendering of page in dev environment. should be further investigated and fixed.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2121387</th>\n",
       "      <td>\"https://github.com/LoungeBuddy/Test/issues/18\"</td>\n",
       "      <td>foursquare api integration with ios app to allow app users access to more lounge reviews and photos.</td>\n",
       "      <td>summary foursquare api integration with ios app to allow app users access to more lounge reviews and photos. steps 1. select lounge. 2. on lounge ui, scroll down to lounge reviews section. 3. next to ‘add a review’ and ‘view all’ buttons, add a foursquare icon/button. 4. on clicking foursquare icon, user is taken directly to the lounge’s page on foursquare. additional notes - numerous airline lounges have extensive reviews on foursquare - location of the foursquare icon is subject to ab test...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2656821</th>\n",
       "      <td>\"https://github.com/SeleniumHQ/selenium/issues/4645\"</td>\n",
       "      <td>dotnet - attempt to save a screenshot to a directory which does not exist throw a cryptic generic gdi error</td>\n",
       "      <td>meta - os: windows selenium version: 3.5.1 browser: any browser version: any when we try to save a screenshot to a directory which does not exist, it throws with the following error: a generic error occurred in gdi+ expected behavior - it should either create the directory or throw an explicit directorynotfoundexception actual behavior - it throws a cryptic generic gdi exception steps to reproduce - add the following to an existing test csharp var testscreenshotpath = c:\\\\pathtoadirectorywhi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                              issue_url  \\\n",
       "618568   \"https://github.com/topcoder-platform/community-app/issues/18\"   \n",
       "2121387                 \"https://github.com/LoungeBuddy/Test/issues/18\"   \n",
       "2656821            \"https://github.com/SeleniumHQ/selenium/issues/4645\"   \n",
       "\n",
       "                                                                                                         issue_title  \\\n",
       "618568                                                                                     font loading issue in dev   \n",
       "2121387         foursquare api integration with ios app to allow app users access to more lounge reviews and photos.   \n",
       "2656821  dotnet - attempt to save a screenshot to a directory which does not exist throw a cryptic generic gdi error   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        body  \n",
       "618568                                                                                                                                                                                                                                                   loading fonts as separate files in dev leads to problems with hmr. i've changed it to load fonts as data urls within js bundle. this solves the problem with hmr, but delays proper rendering of page in dev environment. should be further investigated and fixed.  \n",
       "2121387  summary foursquare api integration with ios app to allow app users access to more lounge reviews and photos. steps 1. select lounge. 2. on lounge ui, scroll down to lounge reviews section. 3. next to ‘add a review’ and ‘view all’ buttons, add a foursquare icon/button. 4. on clicking foursquare icon, user is taken directly to the lounge’s page on foursquare. additional notes - numerous airline lounges have extensive reviews on foursquare - location of the foursquare icon is subject to ab test...  \n",
       "2656821  meta - os: windows selenium version: 3.5.1 browser: any browser version: any when we try to save a screenshot to a directory which does not exist, it throws with the following error: a generic error occurred in gdi+ expected behavior - it should either create the directory or throw an explicit directorynotfoundexception actual behavior - it throws a cryptic generic gdi exception steps to reproduce - add the following to an existing test csharp var testscreenshotpath = c:\\\\pathtoadirectorywhi...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_file='github-issues-data/github_issues.csv'\n",
    "\n",
    "# read in data sample 2000 rows (for speed of tutorial)\n",
    "# Set this to False to train on the entire dataset\n",
    "use_sample_data=True\n",
    "\n",
    "if use_sample_data:\n",
    "    training_data_size=2000\n",
    "    traindf, testdf = train_test_split(pd.read_csv(data_file).sample(n=training_data_size), \n",
    "                                   test_size=.10)\n",
    "else:\n",
    "    traindf, testdf = train_test_split(pd.read_csv(data_file),test_size=.10)\n",
    "\n",
    "\n",
    "#print out stats about shape of data\n",
    "print(f'Train: {traindf.shape[0]:,} rows {traindf.shape[1]:,} columns')\n",
    "print(f'Test: {testdf.shape[0]:,} rows {testdf.shape[1]:,} columns')\n",
    "\n",
    "# preview data\n",
    "traindf.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convert to lists in preparation for modeling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"loading fonts as separate files in dev leads to problems with hmr. i've changed it to load fonts as data urls within js bundle. this solves the problem with hmr, but delays proper rendering of page in dev environment. should be further investigated and fixed.\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_body_raw = traindf.body.tolist()\n",
    "train_title_raw = traindf.issue_title.tolist()\n",
    "#preview output of first element\n",
    "train_body_raw[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Process Data For Deep Learning\n",
    "\n",
    "See [this repo](https://github.com/hamelsmu/ktext) for documentation on the ktext package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from ktext.preprocess import processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:....tokenizing data\n",
      "WARNING:root:(1/2) done. 0 sec\n",
      "WARNING:root:....building corpus\n",
      "WARNING:root:(2/2) done. 0 sec\n",
      "WARNING:root:Finished parsing 1,800 documents.\n",
      "WARNING:root:...fit is finished, beginning transform\n",
      "WARNING:root:...padding data\n",
      "WARNING:root:done. 0 sec\n"
     ]
    }
   ],
   "source": [
    "#%%time\n",
    "# Clean, tokenize, and apply padding / truncating such that each document length = 70\n",
    "#  also, retain only the top 8,000 words in the vocabulary and set the remaining words\n",
    "#  to 1 which will become common index for rare words \n",
    "body_pp = processor(keep_n=8000, padding_maxlen=70)\n",
    "train_body_vecs = body_pp.fit_transform(train_body_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Look at one example of processed issue bodies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "original string:\n",
      " loading fonts as separate files in dev leads to problems with hmr. i've changed it to load fonts as data urls within js bundle. this solves the problem with hmr, but delays proper rendering of page in dev environment. should be further investigated and fixed. \n",
      "\n",
      "after pre-processing:\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0  758 1470   24 1190\n",
      "  103    7  335 1366    4  657   19 4477    6  224  636   10    4  315\n",
      " 1470   24   62 1121  509   60  950   13 4478    3  126   19 4477   25\n",
      " 3503 2170  806   11  115    7  335  247   38   15 1941 4479    9  437] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\noriginal string:\\n', train_body_raw[0], '\\n')\n",
    "print('after pre-processing:\\n', train_body_vecs[0], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:....tokenizing data\n",
      "WARNING:root:(1/2) done. 0 sec\n",
      "WARNING:root:....building corpus\n",
      "WARNING:root:(2/2) done. 0 sec\n",
      "WARNING:root:Finished parsing 1,800 documents.\n",
      "WARNING:root:...fit is finished, beginning transform\n",
      "WARNING:root:...padding data\n",
      "WARNING:root:done. 0 sec\n"
     ]
    }
   ],
   "source": [
    "# Instantiate a text processor for the titles, with some different parameters\n",
    "#  append_indicators = True appends the tokens '_start_' and '_end_' to each\n",
    "#                      document\n",
    "#  padding = 'post' means that zero padding is appended to the end of the \n",
    "#             of the document (as opposed to the default which is 'pre')\n",
    "title_pp = processor(append_indicators=True, keep_n=4500, \n",
    "                     padding_maxlen=12, padding ='post')\n",
    "\n",
    "# process the title data\n",
    "train_title_vecs = title_pp.fit_transform(train_title_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Look at one example of processed issue titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "original string:\n",
      " font loading issue in dev\n",
      "after pre-processing:\n",
      " [  2 255 365  20   6 221   3   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "print('\\noriginal string:\\n', train_title_raw[0])\n",
    "print('after pre-processing:\\n', train_title_vecs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Serialize all of this to disk for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill as dpickle\n",
    "import numpy as np\n",
    "\n",
    "# Save the preprocessor\n",
    "with open('body_pp.dpkl', 'wb') as f:\n",
    "    dpickle.dump(body_pp, f)\n",
    "\n",
    "with open('title_pp.dpkl', 'wb') as f:\n",
    "    dpickle.dump(title_pp, f)\n",
    "\n",
    "# Save the processed data\n",
    "np.save('train_title_vecs.npy', train_title_vecs)\n",
    "np.save('train_body_vecs.npy', train_body_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data from disk into variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seq2seq_utils import load_decoder_inputs, load_encoder_inputs, load_text_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of encoder input: (1800, 70)\n",
      "Shape of decoder input: (1800, 11)\n",
      "Shape of decoder target: (1800, 11)\n"
     ]
    }
   ],
   "source": [
    "encoder_input_data, doc_length = load_encoder_inputs('train_body_vecs.npy')\n",
    "decoder_input_data, decoder_target_data = load_decoder_inputs('train_title_vecs.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary for body_pp.dpkl: 8002\n",
      "Size of vocabulary for title_pp.dpkl: 4254\n"
     ]
    }
   ],
   "source": [
    "num_encoder_tokens, body_pp = load_text_processor('body_pp.dpkl')\n",
    "num_decoder_tokens, title_pp = load_text_processor('title_pp.dpkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, GRU, Dense, Embedding, Bidirectional, BatchNormalization\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#arbitrarly set latent dimension for embedding and hidden units\n",
    "latent_dim = 300\n",
    "\n",
    "##### Define Model Architecture ######\n",
    "\n",
    "########################\n",
    "#### Encoder Model ####\n",
    "encoder_inputs = Input(shape=(doc_length,), name='Encoder-Input')\n",
    "\n",
    "# Word embeding for encoder (ex: Issue Body)\n",
    "x = Embedding(num_encoder_tokens, latent_dim, name='Body-Word-Embedding', mask_zero=False)(encoder_inputs)\n",
    "x = BatchNormalization(name='Encoder-Batchnorm-1')(x)\n",
    "\n",
    "# Intermediate GRU layer (optional)\n",
    "#x = GRU(latent_dim, name='Encoder-Intermediate-GRU', return_sequences=True)(x)\n",
    "#x = BatchNormalization(name='Encoder-Batchnorm-2')(x)\n",
    "\n",
    "# We do not need the `encoder_output` just the hidden state.\n",
    "_, state_h = GRU(latent_dim, return_state=True, name='Encoder-Last-GRU')(x)\n",
    "\n",
    "# Encapsulate the encoder as a separate entity so we can just \n",
    "#  encode without decoding if we want to.\n",
    "encoder_model = Model(inputs=encoder_inputs, outputs=state_h, name='Encoder-Model')\n",
    "\n",
    "seq2seq_encoder_out = encoder_model(encoder_inputs)\n",
    "\n",
    "########################\n",
    "#### Decoder Model ####\n",
    "decoder_inputs = Input(shape=(None,), name='Decoder-Input')  # for teacher forcing\n",
    "\n",
    "# Word Embedding For Decoder (ex: Issue Titles)\n",
    "dec_emb = Embedding(num_decoder_tokens, latent_dim, name='Decoder-Word-Embedding', mask_zero=False)(decoder_inputs)\n",
    "dec_bn = BatchNormalization(name='Decoder-Batchnorm-1')(dec_emb)\n",
    "\n",
    "# Set up the decoder, using `decoder_state_input` as initial state.\n",
    "decoder_gru = GRU(latent_dim, return_state=True, return_sequences=True, name='Decoder-GRU')\n",
    "decoder_gru_output, _ = decoder_gru(dec_bn, initial_state=seq2seq_encoder_out)\n",
    "x = BatchNormalization(name='Decoder-Batchnorm-2')(decoder_gru_output)\n",
    "\n",
    "# Dense layer for prediction\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax', name='Final-Output-Dense')\n",
    "decoder_outputs = decoder_dense(x)\n",
    "\n",
    "########################\n",
    "#### Seq2Seq Model ####\n",
    "\n",
    "#seq2seq_decoder_out = decoder_model([decoder_inputs, seq2seq_encoder_out])\n",
    "seq2seq_Model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "\n",
    "seq2seq_Model.compile(optimizer=optimizers.Nadam(lr=0.001), loss='sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Examine Model Architecture Summary **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Decoder-Input (InputLayer)      (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-Word-Embedding (Embeddi (None, None, 300)    1276200     Decoder-Input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-Input (InputLayer)      (None, 70)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-Batchnorm-1 (BatchNorma (None, None, 300)    1200        Decoder-Word-Embedding[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-Model (Model)           (None, 300)          2942700     Encoder-Input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-GRU (GRU)               [(None, None, 300),  540900      Decoder-Batchnorm-1[0][0]        \n",
      "                                                                 Encoder-Model[1][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-Batchnorm-2 (BatchNorma (None, None, 300)    1200        Decoder-GRU[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Final-Output-Dense (Dense)      (None, None, 4254)   1280454     Decoder-Batchnorm-2[0][0]        \n",
      "==================================================================================================\n",
      "Total params: 6,042,654\n",
      "Trainable params: 6,040,854\n",
      "Non-trainable params: 1,800\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"547pt\" viewBox=\"0.00 0.00 547.00 410.00\" width=\"729pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1.3333 1.3333) rotate(0) translate(4 406)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"#ffffff\" points=\"-4,4 -4,-406 543,-406 543,4 -4,4\" stroke=\"transparent\"/>\n",
       "<!-- 140648840725224 -->\n",
       "<g class=\"node\" id=\"node1\">\n",
       "<title>140648840725224</title>\n",
       "<polygon fill=\"none\" points=\"59,-365.5 59,-401.5 268,-401.5 268,-365.5 59,-365.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"163.5\" y=\"-379.8\">Decoder-Input: InputLayer</text>\n",
       "</g>\n",
       "<!-- 140647629199904 -->\n",
       "<g class=\"node\" id=\"node2\">\n",
       "<title>140647629199904</title>\n",
       "<polygon fill=\"none\" points=\"15.5,-292.5 15.5,-328.5 311.5,-328.5 311.5,-292.5 15.5,-292.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"163.5\" y=\"-306.8\">Decoder-Word-Embedding: Embedding</text>\n",
       "</g>\n",
       "<!-- 140648840725224&#45;&gt;140647629199904 -->\n",
       "<g class=\"edge\" id=\"edge1\">\n",
       "<title>140648840725224-&gt;140647629199904</title>\n",
       "<path d=\"M163.5,-365.4551C163.5,-357.3828 163.5,-347.6764 163.5,-338.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"167.0001,-338.5903 163.5,-328.5904 160.0001,-338.5904 167.0001,-338.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140648030401256 -->\n",
       "<g class=\"node\" id=\"node4\">\n",
       "<title>140648030401256</title>\n",
       "<polygon fill=\"none\" points=\"0,-219.5 0,-255.5 327,-255.5 327,-219.5 0,-219.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"163.5\" y=\"-233.8\">Decoder-Batchnorm-1: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 140647629199904&#45;&gt;140648030401256 -->\n",
       "<g class=\"edge\" id=\"edge2\">\n",
       "<title>140647629199904-&gt;140648030401256</title>\n",
       "<path d=\"M163.5,-292.4551C163.5,-284.3828 163.5,-274.6764 163.5,-265.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"167.0001,-265.5903 163.5,-255.5904 160.0001,-265.5904 167.0001,-265.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140648027174952 -->\n",
       "<g class=\"node\" id=\"node3\">\n",
       "<title>140648027174952</title>\n",
       "<polygon fill=\"none\" points=\"330,-292.5 330,-328.5 539,-328.5 539,-292.5 330,-292.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"434.5\" y=\"-306.8\">Encoder-Input: InputLayer</text>\n",
       "</g>\n",
       "<!-- 140650169865664 -->\n",
       "<g class=\"node\" id=\"node5\">\n",
       "<title>140650169865664</title>\n",
       "<polygon fill=\"none\" points=\"345,-219.5 345,-255.5 524,-255.5 524,-219.5 345,-219.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"434.5\" y=\"-233.8\">Encoder-Model: Model</text>\n",
       "</g>\n",
       "<!-- 140648027174952&#45;&gt;140650169865664 -->\n",
       "<g class=\"edge\" id=\"edge3\">\n",
       "<title>140648027174952-&gt;140650169865664</title>\n",
       "<path d=\"M434.5,-292.4551C434.5,-284.3828 434.5,-274.6764 434.5,-265.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"438.0001,-265.5903 434.5,-255.5904 431.0001,-265.5904 438.0001,-265.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140647935041720 -->\n",
       "<g class=\"node\" id=\"node6\">\n",
       "<title>140647935041720</title>\n",
       "<polygon fill=\"none\" points=\"220,-146.5 220,-182.5 377,-182.5 377,-146.5 220,-146.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"298.5\" y=\"-160.8\">Decoder-GRU: GRU</text>\n",
       "</g>\n",
       "<!-- 140648030401256&#45;&gt;140647935041720 -->\n",
       "<g class=\"edge\" id=\"edge4\">\n",
       "<title>140648030401256-&gt;140647935041720</title>\n",
       "<path d=\"M196.8708,-219.4551C214.8007,-209.7596 237.0905,-197.7066 256.2227,-187.361\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"257.9137,-190.4257 265.0452,-182.5904 254.5841,-184.2682 257.9137,-190.4257\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140650169865664&#45;&gt;140647935041720 -->\n",
       "<g class=\"edge\" id=\"edge5\">\n",
       "<title>140650169865664-&gt;140647935041720</title>\n",
       "<path d=\"M400.882,-219.4551C382.8193,-209.7596 360.3644,-197.7066 341.0904,-187.361\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"342.6689,-184.236 332.2026,-182.5904 339.3582,-190.4037 342.6689,-184.236\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140647657573064 -->\n",
       "<g class=\"node\" id=\"node7\">\n",
       "<title>140647657573064</title>\n",
       "<polygon fill=\"none\" points=\"135,-73.5 135,-109.5 462,-109.5 462,-73.5 135,-73.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"298.5\" y=\"-87.8\">Decoder-Batchnorm-2: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 140647935041720&#45;&gt;140647657573064 -->\n",
       "<g class=\"edge\" id=\"edge6\">\n",
       "<title>140647935041720-&gt;140647657573064</title>\n",
       "<path d=\"M298.5,-146.4551C298.5,-138.3828 298.5,-128.6764 298.5,-119.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"302.0001,-119.5903 298.5,-109.5904 295.0001,-119.5904 302.0001,-119.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140647675731248 -->\n",
       "<g class=\"node\" id=\"node8\">\n",
       "<title>140647675731248</title>\n",
       "<polygon fill=\"none\" points=\"193.5,-.5 193.5,-36.5 403.5,-36.5 403.5,-.5 193.5,-.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"298.5\" y=\"-14.8\">Final-Output-Dense: Dense</text>\n",
       "</g>\n",
       "<!-- 140647657573064&#45;&gt;140647675731248 -->\n",
       "<g class=\"edge\" id=\"edge7\">\n",
       "<title>140647657573064-&gt;140647675731248</title>\n",
       "<path d=\"M298.5,-73.4551C298.5,-65.3828 298.5,-55.6764 298.5,-46.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"302.0001,-46.5903 298.5,-36.5904 295.0001,-46.5904 302.0001,-46.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from seq2seq_utils import viz_model_architecture\n",
    "seq2seq_Model.summary()\n",
    "viz_model_architecture(seq2seq_Model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1584 samples, validate on 216 samples\n",
      "Epoch 1/7\n",
      "1584/1584 [==============================] - 7s 4ms/step - loss: 8.1897 - val_loss: 7.1236\n",
      "Epoch 2/7\n",
      "1584/1584 [==============================] - 6s 4ms/step - loss: 6.9765 - val_loss: 6.4204\n",
      "Epoch 3/7\n",
      "1584/1584 [==============================] - 6s 4ms/step - loss: 6.2519 - val_loss: 5.9892\n",
      "Epoch 4/7\n",
      "1584/1584 [==============================] - 6s 4ms/step - loss: 5.7648 - val_loss: 5.6660\n",
      "Epoch 5/7\n",
      "1584/1584 [==============================] - 6s 4ms/step - loss: 5.3693 - val_loss: 5.4416\n",
      "Epoch 6/7\n",
      "1584/1584 [==============================] - 6s 4ms/step - loss: 5.0281 - val_loss: 5.2472\n",
      "Epoch 7/7\n",
      "1584/1584 [==============================] - 6s 4ms/step - loss: 4.7387 - val_loss: 5.1499\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import CSVLogger, ModelCheckpoint\n",
    "\n",
    "script_name_base = 'tutorial_seq2seq'\n",
    "csv_logger = CSVLogger('{:}.log'.format(script_name_base))\n",
    "model_checkpoint = ModelCheckpoint('{:}.epoch{{epoch:02d}}-val{{val_loss:.5f}}.hdf5'.format(script_name_base),\n",
    "                                   save_best_only=True)\n",
    "\n",
    "batch_size = 1200\n",
    "epochs = 7\n",
    "history = seq2seq_Model.fit([encoder_input_data, decoder_input_data], np.expand_dims(decoder_target_data, -1),\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.12, callbacks=[csv_logger, model_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model\n",
    "seq2seq_Model.save('seq2seq_model_tutorial.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# See Example Results On Holdout Set\n",
    "\n",
    "It is useful to see examples of real predictions on a holdout set to get a sense of the performance of the model.  We will also evaluate the model numerically in a following section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seq2seq_utils import Seq2Seq_Inference\n",
    "seq2seq_inf = Seq2Seq_Inference(encoder_preprocessor=body_pp,\n",
    "                                 decoder_preprocessor=title_pp,\n",
    "                                 seq2seq_model=seq2seq_Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 161 =================\n",
      "\n",
      "\"https://github.com/dyweb/gommon/issues/7\"\n",
      "Issue Body:\n",
      " found it when using prometheus - https://github.com/prometheus/common/blob/master/log/log.go l240 \n",
      "\n",
      "Original Title:\n",
      " log support source line number\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " make fix crash in\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 21 =================\n",
      "\n",
      "\"https://github.com/guyb7/time-prices/issues/5\"\n",
      "Issue Body:\n",
      " unnecessary listeners are left behind on sites with constant dom changes single-page-apps , may cause memory leaks. \n",
      "\n",
      "Original Title:\n",
      " remove listeners from destroyed elements\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " first from flow in uk south\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 26 =================\n",
      "\n",
      "\"https://github.com/exercism/xlua/issues/187\"\n",
      "Issue Body:\n",
      " this appears to be due to a point update 5.3.3 => 5.3.4 changing a check around __tostring . \n",
      "\n",
      "Original Title:\n",
      " minesweeper sample fails ci\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " add not working\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 164 =================\n",
      "\n",
      "\"https://github.com/dalaranwow/dalaran-wow/issues/5135\"\n",
      "Issue Body:\n",
      " // : information about tags // : the black and yellow tag is unique. there can only be one label assigned to them - one black and one yellow // : priority - ... tags are assigned by team members // : your issue will be reviewed by testers and the developers will fix it in the future. // : only once the issue got the label fixed - on live server you can find the fix on your gameserver. // : you can increase the relevance of your issue by adding a thumb-up emote http://i.imgur.com/rfouvvi.png // : // : !!!!!!!!!!!!!don't delete this template else your issue will be closed!!!!!!!!!!!! // : current behaviour : gnaw is on global cd wich wich does not allow you to use it when you want but instead you have to time it right between pets gcd aka spam it before ghoul uses his abilities. expected behaviour : gnaw should not trigger ghoul’s gcd. this bug makes it hard to use corpse explosion, because if u cast ce while pet is on gcd, it won’t go off and u will waste 40 runic power. steps to reproduce the problem : 1. attack your target with a pet. use claw and observe that gnaw triggers gcd 2. 3. include proofs for this behaviour https://www.youtube.com/watch?v=moppsucgrny http://www.warcraftmovies.com/movieview.php?id=160059 same video as previous link time 3:13, 18:05 watch pet's gcd. the dk is able to cast ce right after using gnaw. https://youtu.be/qxr5smkcn08?t=1565 related issue: https://github.com/dalaranwow/dalaran-wow/issues/3548 \n",
      "\n",
      "Original Title:\n",
      " gnaw ghoul stun and ghoul's gcd\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " add not working data\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 38 =================\n",
      "\n",
      "\"https://github.com/reactioncommerce/reaction/issues/2291\"\n",
      "Issue Body:\n",
      " expected behavior i actually don't 100% know what's supposed to happen i assume a share this but i don't think an error is what we expect actual behavior error in the pop-up browser ! error_and_dumb_product_ https://cloud.githubusercontent.com/assets/72819/26133236/470abd72-3ad8-11e7-9bea-38c2f17449ed.png steps to reproduce the behavior 1. configure facebook sharing in the dashboard 1, sign into fb or sign in when prompted later 1. create a product 1. publish that product 1. go to the pdp 1. click on the fb icon 1. observe the error message versions node: 7.10.0 npm: 4.2.0 meteor node: 4.8.2 meteor npm: 4.5.0 reaction cli: 0.8.2 reaction: 1.2.0 reaction branch: release-1.2.0 docker: 17.03.1-ce \n",
      "\n",
      "Original Title:\n",
      " pdp receive an error when clicking on fb icon in the pdp\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " add not working\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 75 =================\n",
      "\n",
      "\"https://github.com/DrupalRu/drupal.ru/issues/646\"\n",
      "Issue Body:\n",
      " аж с 2015 года висят. нужно срочно назначить ответственного за пересмотр их актуальности и позакрывать ненужные. я бы сделал это первостепенной задачей, а то 100+ issue меня напрягает. в частности закрыть те, которые относились, например. к старому дизайну. они точно уже неактуальны . вроде @awd-studio собирался разбирать их \n",
      "\n",
      "Original Title:\n",
      " слишком много открытых issue\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " make fix crash in\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 141 =================\n",
      "\n",
      "\"https://github.com/STSSoft/STSdb4/issues/5\"\n",
      "Issue Body:\n",
      " hi, i came up with some tests of your database implementation and find out that every lookup operation performs really bad on big data. it seems that nearly 40% of the time is taken by task.wait . to check the performance you can easily extend your getting-started example by the following block: csharp //lookup console.writeline lookup... ; sw.reset ; c = 0; using istorageengine engine = stsdb.fromfile file_name { itable<long, tick> table = engine.openxtable<long, tick> table ; //select all ids to be sure there is a match var ids = table.select kvp => kvp.key .tolist ; //loading keys is not part of measurement sw.start ; foreach var id in ids //table.forward , table.backward { var data = table.find id ; //console.writeline {0} {1} , id, data ; c++; if c % 100000 == 0 console.writeline found {0} records , c ; } } sw.stop ; console.writeline lookup speed:{0} rec/sec , sw.getspeed c ; do you have any plans on increasing lookup performance? \n",
      "\n",
      "Original Title:\n",
      " lookup operation performance is bad\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " first from flow in uk south\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 136 =================\n",
      "\n",
      "\"https://github.com/VSCodeVim/Vim/issues/1731\"\n",
      "Issue Body:\n",
      " <!-- for questions, ask us on slack https://vscodevim-slackin.azurewebsites.net/ 👫. dont change anything until the -----. thanks! --> click thumbs-up 👍 on this issue if you want it! click confused 😕 on this issue if not having it makes vscodevim unusable. the vscodevim team prioritizes issues based on reaction count. -------- bug report : typing a quote in a snippet and then typing after that does not replace the quoted contents. example: start: ! image https://cloud.githubusercontent.com/assets/356714/26274453/4f786454-3cff-11e7-8916-9e1439706833.png start -> after typing and f : ! image https://cloud.githubusercontent.com/assets/356714/26274457/594674da-3cff-11e7-9386-d14a8496aada.png ! image https://cloud.githubusercontent.com/assets/356714/26274494/e9225006-3cff-11e7-8aae-ebfc3ad7b8e8.png expected when vscodevim is disabled : iscachevalid f start -> after typing f : ! image https://cloud.githubusercontent.com/assets/356714/26274461/679174d6-3cff-11e7-9f2e-1aa21a245682.png - vscode version : 1.12, 1.13 insiders - vscodevim version : 0.7.1 - os : win10 \n",
      "\n",
      "Original Title:\n",
      " typing a quote in a snippet and then typing after that does not replace the quoted contents.\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " allow add not working\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 77 =================\n",
      "\n",
      "\"https://github.com/eddyerburgh/avoriaz/issues/61\"\n",
      "Issue Body:\n",
      " is this an expected behaviour? in that case? how can i stub child component without loosing the execution of created hook? thank you \n",
      "\n",
      "Original Title:\n",
      " shallow does not reach created hook\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " add not working\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 181 =================\n",
      "\n",
      "\"https://github.com/tomkersten/ghost-docker/issues/3\"\n",
      "Issue Body:\n",
      " the docker container does not exit/stop. it should. \n",
      "\n",
      "Original Title:\n",
      " bin/install does not kill/stop container when done\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " first from flow in uk south\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 47 =================\n",
      "\n",
      "\"https://github.com/gradle-guides/building-java-9-modules/issues/4\"\n",
      "Issue Body:\n",
      " we really should have to explain that the newly patched module is going to read the junit automatic module, but we don't have to for leaf projects like the actors project. the jigsaw-dev mailing list needs to know. \n",
      "\n",
      "Original Title:\n",
      " file the bug with jigsaw-dev about leaf modules and --add-reads\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " first from flow in uk south\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 172 =================\n",
      "\n",
      "\"https://github.com/gin-gonic/gin/issues/1007\"\n",
      "Issue Body:\n",
      " var server gin.engine = gin.default server.use func c gin.context { if true { // if error c.string 401, insufficient permissions c.abort } else { c.next } } //when the client to upload files test size more than >1m , middleware cannot normal response \n",
      "\n",
      "Original Title:\n",
      " when the client to upload files, middleware can't work normally.\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " fix crash in number\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 129 =================\n",
      "\n",
      "\"https://github.com/Mrigank11/embetacloud/issues/27\"\n",
      "Issue Body:\n",
      " google_redirect_url it should be <server address>/oauthcallback by default i just set http://domain.com/oauthcallback as redirect url but comes to end like http://domain.com/oauthcallback?code=4/ii1zorld5fxdp_xbdcnui1vtbcx2qq7dcjeik3orggg shows error \n",
      "\n",
      "Original Title:\n",
      " google drive problem\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " add add not working\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 60 =================\n",
      "\n",
      "\"https://github.com/zigbeer/zigbee-shepherd/issues/8\"\n",
      "Issue Body:\n",
      " hello, when i try to connect some devices it happens with both the smartthings multisensor and the xiaomi mi smart home wireless switch i will recieve this error cc-znp:areq <-- zdo:tcdeviceind, { nwkaddr: 6556, extaddr: '0x00158d0001549d8b', parentaddr: 0 } +13s cc-znp:areq <-- zdo:enddeviceannceind, { srcaddr: 6556, nwkaddr: 6556, ieeeaddr: '0x00158d0001549d8b', capabilities: 128 } +364ms cc-znp:sreq --> zdo:nodedescreq, { dstaddr: 6556, nwkaddrofinterest: 6556 } +1ms cc-znp:srsp <-- zdo:nodedescreq, { status: 0 } +10ms cc-znp:sreq --> zdo:nodedescreq, { dstaddr: 6556, nwkaddrofinterest: 6556 } +10s cc-znp:srsp <-- zdo:nodedescreq, { status: 0 } +10ms cannot get the node descriptor of the device: 0x00158d0001549d8b cannot get the node descriptor of the device: 0x00158d0001549d8b the error appears to be caused by a timeout when sending controller.request 'zdo', 'nodedescreq', { dstaddr: nwkaddr, nwkaddrofinterest: nwkaddr } looking at the code, it appears the device is not responding, however both of these devices have been confirmed as working, and worked flawlessly with their respective hubs thank you, - nick remijn \n",
      "\n",
      "Original Title:\n",
      " failure to connect with some devices\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " first from flow in uk south\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 163 =================\n",
      "\n",
      "\"https://github.com/18F/fec-transition/issues/161\"\n",
      "Issue Body:\n",
      " so users can access classic.fec.gov after we shutdown our waltham data center, we should host classic.fec.gov in an in alternative hosting center. \n",
      "\n",
      "Original Title:\n",
      " create environment for classic.fec.gov\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " make add not working\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 28 =================\n",
      "\n",
      "\"https://github.com/ksAutotests/CreateInvalidAndUpdateInvalidTest/issues/340\"\n",
      "Issue Body:\n",
      " tutorial issue found: https://github.com/ksautotests/createinvalidandupdateinvalidtest/blob/master/tutorials/firefox/tutorial_firefox.md https://github.com/ksautotests/createinvalidandupdateinvalidtest/blob/master/tutorials/firefox/tutorial_firefox.md contains invalid primary tag.\n",
      "your tutorial was not created. please double-check primary tag property. each tutorial md-file shall have primary tag provided above. example:\n",
      "\\-\\-\\-\n",
      "title: text bundles within node.js sap hana applications\n",
      "description: working with text bundles in node.js\n",
      "primary_tag: products>sap\\-hana\n",
      "tags: tutorial>intermediate\\, products>sap\\-hana\\, products>sap\\-hana\\-\\-express\\-edition \\-\\-\\- affected server: test blue \n",
      "\n",
      "Original Title:\n",
      " tutorial page tutorial_firefox.md issue. test blue\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " add not working\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 17 =================\n",
      "\n",
      "\"https://github.com/quasarframework/quasar/issues/371\"\n",
      "Issue Body:\n",
      " i'm not sure if this should be an issue or not; but if you lay out two side-by-side hr's, the edges of them curve a little and don't line up. ! image https://cloud.githubusercontent.com/assets/3268854/22615199/b58b7b5e-ea5d-11e6-8d13-867b74d72fef.png would there be a real negative impact from just making the hr's a straight line, vs curving at the edges? in the docs http://quasar-framework.org/guide/app-pre-processors.html using-pre-processors-inside-components it's just a bottom border, so i'm wondering if there should really be a different or not. \n",
      "\n",
      "Original Title:\n",
      " ui hr's don't align well\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " add not working\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 187 =================\n",
      "\n",
      "\"https://github.com/neighbourhoodie/voice-of-interconnect/issues/18\"\n",
      "Issue Body:\n",
      " setup using webpack. this should allow us to use build the frontend app more easily and more modular and give us things like live reloads \n",
      "\n",
      "Original Title:\n",
      " npm run dev for app development\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " first from flow in uk south\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 84 =================\n",
      "\n",
      "\"https://github.com/weenhanceit/autism-funding/issues/38\"\n",
      "Issue Body:\n",
      " another problem noted, but not followed up on - an invoice was assigned to an rtp - but shows in the summary as out of pocket. \n",
      "\n",
      "Original Title:\n",
      " invoice assigned to rtp but shows in summary as out of pocket\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " add not working\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 58 =================\n",
      "\n",
      "\"https://github.com/careteditor/caret/issues/279\"\n",
      "Issue Body:\n",
      " when i create a link to a non-existing markdown file and click the link in the preview, caret displays an error message: ! image https://cloud.githubusercontent.com/assets/1894528/24491077/d1e3d6c8-1525-11e7-9df0-fc406a2b63df.png it would be a huge time saver if caret would instead or additionally offer to create the file instead and then navigate there. \n",
      "\n",
      "Original Title:\n",
      " feature request: create file when navigating to a non-existing link\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " add not working\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 139 =================\n",
      "\n",
      "\"https://github.com/salsify/safer_rails_console/issues/16\"\n",
      "Issue Body:\n",
      " model.first triggers a rollback, however model.where 'invalid statement' does not which ends up poisoning the transaction. \n",
      "\n",
      "Original Title:\n",
      " invalid cached and prepared statements do not trigger a automatic rollback with postgresql\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " add allow not working\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 193 =================\n",
      "\n",
      "\"https://github.com/google/google-api-go-client/issues/217\"\n",
      "Issue Body:\n",
      " i'm trying to use the drive/v3 api for partial downloads. 'range: bytes 123-456' as mentioned here: https://developers.google.com/drive/v3/web/manage-downloads partial_download isn't it possible to add the range in the standard call? resp, err := gdclient.files.get docid .download .? if not, is the only way to do this by pulling the download url, add token and range request header? \n",
      "\n",
      "Original Title:\n",
      " drive/v3 with range: partial downloads\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " first from flow in uk south\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 113 =================\n",
      "\n",
      "\"https://github.com/balderdashy/skipper/issues/174\"\n",
      "Issue Body:\n",
      " this issue was discovered when using http-proxy-middleware after the bodyparser stage, and it only affects delete requests. in _parsehttpbody , the parsing falls through to the re-run of the json bodyparser. it takes a copy of req.headers 'content-type' - but this is undefined. parsing fails again, so the line req.headers 'content-type' = backupcontenttype; has now set the content-type header to undefined. this blows up inside node's _http_outgoing.js:outgoingmessage.prototype.setheader if value === undefined throw new error ' value required in setheader ' + name + ' , value ' ; when http-proxy-middleware tries to do its thing. suggested patch: diff --- a/node_modules/skipper/index.js +++ b/node_modules/skipper/index.js @@ -133,7 +133,11 @@ module.exports = function toparsehttpbody options { // revert content-type to what it originally was. // this is so we don't inadvertently corrupt req.headers -- // our apps' actions might be looking for 'em. - req.headers 'content-type' = backupcontenttype; + if backupcontenttype { + req.headers 'content-type' = backupcontenttype; + } else { + delete req.headers 'content-type' ; + } // if an error occurred in the retry, it's not actually an error // we can't assume every requeset was intended to be json \n",
      "\n",
      "Original Title:\n",
      " skipper inadvertently corrupts req.headers when it can't parse the body\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " add not working data\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 139 =================\n",
      "\n",
      "\"https://github.com/salsify/safer_rails_console/issues/16\"\n",
      "Issue Body:\n",
      " model.first triggers a rollback, however model.where 'invalid statement' does not which ends up poisoning the transaction. \n",
      "\n",
      "Original Title:\n",
      " invalid cached and prepared statements do not trigger a automatic rollback with postgresql\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " add allow not working\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 23 =================\n",
      "\n",
      "\"https://github.com/Instagram/IGListKit/issues/1040\"\n",
      "Issue Body:\n",
      " we recently launched a new docs and site gen tool https://docusaurus.io/ called docusaurus. i've gotten some previews of the tool and its pretty awesome. i've been wanted to get a standalone site for iglistkit setup for a while. working w/ some fb folks on logistics, but in the meantime there are a few things we need to sort out: - we gen api docs with jazzy using our headers. i'd love to still use this b/c its so convenient and integrates w/ xcode really well. i'd like to avoid having two distinct types of docs api w/ jazzy and general w/ docusaurus . if we can extend jazzy to export docusaurus docs, that'd be amazing. - design! - we've talked about making some new tutorials and even videos. stay tuned! cc @manicakes \n",
      "\n",
      "Original Title:\n",
      " setup a docusaurus site\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " first from flow in uk south\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 132 =================\n",
      "\n",
      "\"https://github.com/idaholab/moose/issues/8644\"\n",
      "Issue Body:\n",
      " description of the enhancement or error report have 2 variables/postprocessors, a and b . hide a via hide = 'a' and show b via show = 'b' and you will get output s specified to be both shown and hidden: a rationale for the enhancement or information for reproducing the error input file like so triggers the error for variables. mesh type = generatedmesh dim = 2 xmin = 0 xmax = 1 ymin = 0 ymax = 1 nx = 2 ny = 2 elem_type = quad4 functions ./ffn type = parsedfunction value = -4 ../ ./exactfn type = parsedfunction value = x x+y y ../ variables ./u order = first family = lagrange ../ ./v order = first family = lagrange ../ kernels ./td type = timederivative variable = u ../ ./diff type = diffusion variable = u ../ ./force type = userforcingfunction variable = u function = ffn ../ ./td_v type = timederivative variable = v ../ ./diff_v type = diffusion variable = v ../ ./force_v type = userforcingfunction variable = v function = ffn ../ bcs ./all_u type = functiondirichletbc variable = u boundary = '0 1 2 3' function = exactfn ../ ./all_v type = functiondirichletbc variable = v boundary = '0 1 2 3' function = exactfn ../ executioner type = transient solve_type = 'pjfnk' dt = 0.01 start_time = 0 num_steps = 1 outputs console = true ./out type = exodus show = 'u' hide = 'v' ../ identified impact i.e. internal object changes, limited interface changes, public api change, or a list of specific applications impacted correct behavior or show / hide . \n",
      "\n",
      "Original Title:\n",
      " show and hide output params used together produce incorrect error message\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " fix crash number\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 193 =================\n",
      "\n",
      "\"https://github.com/google/google-api-go-client/issues/217\"\n",
      "Issue Body:\n",
      " i'm trying to use the drive/v3 api for partial downloads. 'range: bytes 123-456' as mentioned here: https://developers.google.com/drive/v3/web/manage-downloads partial_download isn't it possible to add the range in the standard call? resp, err := gdclient.files.get docid .download .? if not, is the only way to do this by pulling the download url, add token and range request header? \n",
      "\n",
      "Original Title:\n",
      " drive/v3 with range: partial downloads\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " first from flow in uk south\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 86 =================\n",
      "\n",
      "\"https://github.com/kenzanlabs/kubernetes-ci-cd/issues/22\"\n",
      "Issue Body:\n",
      " some of the commands embedded in the tutorial are not really well formatted for cutting and running on a linux machine. for example, i think this; sudo docker stop socat-registry && sudo docker rm socat-registry is better than; docker stop socat-registry; docker rm socat-registry; \n",
      "\n",
      "Original Title:\n",
      " command line examples missing \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " add not working\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 22 =================\n",
      "\n",
      "\"https://github.com/skeeto/elfeed/issues/204\"\n",
      "Issue Body:\n",
      " today i accidently pressed c-x w and g to download new content while wifi was offf and it returned successfully. i was hoping for a message or a beep. \n",
      "\n",
      "Original Title:\n",
      " elfeed should show some kind of error notification if the remote feed url was not accessible\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " make fix crash in\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 127 =================\n",
      "\n",
      "\"https://github.com/vividvilla/csvtotable/issues/25\"\n",
      "Issue Body:\n",
      " would it be possible for the next version to include in the final html a line <!-- this file was made by csvtotable https://github.com/vividvilla/csvtotable --> ? knowlegeable visitors to my pages have been asking me how it was done ... \n",
      "\n",
      "Original Title:\n",
      " this file was made by\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " first from flow in uk south\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 176 =================\n",
      "\n",
      "\"https://github.com/OpenTreeOfLife/reference-taxonomy/issues/318\"\n",
      "Issue Body:\n",
      " right now we use some very ad hoc regular expressions. using gnparse would be much nicer. here is a code snippet proving that you can invoke gnparse from jython thanks @dimus : curl -l https://github.com/globalnamesarchitecture/gnparser/releases/download/release-0.3.3/gnparser-assembly-0.3.3.jar >gnparser-assembly-0.3.3.jar curl -l http://mumble.net/~jar/tmp/jython-standalone.jar >jython-standalone.jar cat >foo.py <<eof from org.globalnames.parser import scientificnameparser import json z = scientificnameparser.instance .fromstring homo sapiens l. print z.canonized false .get eof export jythonpath=gnparser-assembly-0.3.3.jar jython=jython-standalone.jar java -jar $jython foo.py \n",
      "\n",
      "Original Title:\n",
      " use gnparse instead of regular expressions to convert gbif scientificname column to canonicalname\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " first from flow in uk south\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 27 =================\n",
      "\n",
      "\"https://github.com/datacite/datacite/issues/263\"\n",
      "Issue Body:\n",
      " currently we have sporadic insight into data for the health of api end points, it would be good to gather more data in order to be able to make better architectural decisions. main goals to get things like no of requests status codes avg request time. note some of this may be available already but bring together in this issue to figure out where and how this should be found going forward. \n",
      "\n",
      "Original Title:\n",
      " api health checks\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " add add not working\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 179 =================\n",
      "\n",
      "\"https://github.com/andybotting/xbmc-addon-abc-iview/issues/2579\"\n",
      "Issue Body:\n",
      " automatic bug report from end-user. environment plugin name: abc iview plugin id: plugin.video.abc_iview plugin version: 1.8.0 xbmc/kodi version: 15.0 git:2015-07-21-2f34a0c python version: 2.6.5 r265:79063, jul 21 2015, 20:08:01 gcc 4.8 operating system: linux3 linux 3.10.33 armv7l ip address: 58.179.185.146 146.185.179.58.sta.dodo.net.au isp : primus telecommunications python path: /storage/emulated/0/android/data/org.xbmc.kodi/files/.kodi/addons/plugin.video.abc_iview\n",
      "/storage/emulated/0/android/data/org.xbmc.kodi/files/.kodi/addons/script.common.plugin.cache/lib\n",
      "/storage/emulated/0/android/data/org.xbmc.kodi/files/.kodi/addons/script.module.beautifulsoup/lib\n",
      "/storage/emulated/0/android/data/org.xbmc.kodi/files/.kodi/addons/script.module.requests/lib\n",
      "/\n",
      "/data/app/org.xbmc.kodi-1.apk/assets/python2.6/lib/python26.zip\n",
      "/data/app/org.xbmc.kodi-1.apk/assets/python2.6/lib/python2.6\n",
      "/data/app/org.xbmc.kodi-1.apk/assets/python2.6/lib/python2.6/plat-linux3\n",
      "/data/app/org.xbmc.kodi-1.apk/assets/python2.6/lib/python2.6/lib-tk\n",
      "/data/app/org.xbmc.kodi-1.apk/assets/python2.6/lib/python2.6/lib-old\n",
      "/data/app/org.xbmc.kodi-1.apk/assets/python2.6/lib/python2.6/lib-dynload\n",
      "/storage/emulated/0/android/data/org.xbmc.kodi/files/.kodi/addons/plugin.video.abc_iview/resources/lib traceback traceback most recent call last : file /storage/emulated/0/android/data/org.xbmc.kodi/files/.kodi/addons/plugin.video.abc_iview/resources/lib/play.py , line 45, in play p.parse_xbmc_url url file /storage/emulated/0/android/data/org.xbmc.kodi/files/.kodi/addons/plugin.video.abc_iview/resources/lib/classes.py , line 358, in parse_xbmc_url timestamp = time.mktime time.strptime d 'date' , '%y-%m-%d %h:%m:%s' importerror: failed to import _strptime because the import lockis held by another thread. full xbmc.log https://gist.github.com/06bb382f9b8ca6da2a5a02924f806563 \n",
      "\n",
      "Original Title:\n",
      " end-user bug report\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " add not working\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 38 =================\n",
      "\n",
      "\"https://github.com/reactioncommerce/reaction/issues/2291\"\n",
      "Issue Body:\n",
      " expected behavior i actually don't 100% know what's supposed to happen i assume a share this but i don't think an error is what we expect actual behavior error in the pop-up browser ! error_and_dumb_product_ https://cloud.githubusercontent.com/assets/72819/26133236/470abd72-3ad8-11e7-9bea-38c2f17449ed.png steps to reproduce the behavior 1. configure facebook sharing in the dashboard 1, sign into fb or sign in when prompted later 1. create a product 1. publish that product 1. go to the pdp 1. click on the fb icon 1. observe the error message versions node: 7.10.0 npm: 4.2.0 meteor node: 4.8.2 meteor npm: 4.5.0 reaction cli: 0.8.2 reaction: 1.2.0 reaction branch: release-1.2.0 docker: 17.03.1-ce \n",
      "\n",
      "Original Title:\n",
      " pdp receive an error when clicking on fb icon in the pdp\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " add not working\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 43 =================\n",
      "\n",
      "\"https://github.com/James-Yu/LaTeX-Workshop/issues/173\"\n",
      "Issue Body:\n",
      " i am only able to get the citation browser to respond i.e., display anything beyond it's default display contents to the letter a. any other letter, or any further letters, has no effect. let me know if i should provide a screenshot. \n",
      "\n",
      "Original Title:\n",
      " citation browser broken\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " first from flow in uk south\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 97 =================\n",
      "\n",
      "\"https://github.com/mozilla-services/syncserver/issues/92\"\n",
      "Issue Body:\n",
      " just like : https://accounts.firefox.com/ver.json cc @jbuck @jrgm \n",
      "\n",
      "Original Title:\n",
      " provide a /__version__ endpoint and return the current running version\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " make add not working\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 6 =================\n",
      "\n",
      "\"https://github.com/Azure/azure-powershell/issues/3831\"\n",
      "Issue Body:\n",
      " give users the ability to skip help generation when building locally due to the large amount of time it can take to create the maml files. follow a model similar to codesign https://github.com/azure/azure-powershell/blob/preview/build.proj l16 when adding the switch for skiphelp \n",
      "\n",
      "Original Title:\n",
      " add switch to allow skipping help generation during build\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " add not working number\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 107 =================\n",
      "\n",
      "\"https://github.com/hmlON/room538/issues/9\"\n",
      "Issue Body:\n",
      " fade out notification after 5 seconds \n",
      "\n",
      "Original Title:\n",
      " notification fade out\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " add not working\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 8 =================\n",
      "\n",
      "\"https://github.com/sarahholden/fundamentals/issues/99\"\n",
      "Issue Body:\n",
      " pretty sure this is a remnant of the original readme, and is a nonsequitur. i'll remove the timing. \n",
      "\n",
      "Original Title:\n",
      " ch8_loops for loop 10 mins\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " add allow not working\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 88 =================\n",
      "\n",
      "\"https://github.com/commixproject/commix/issues/81\"\n",
      "Issue Body:\n",
      " the title says it all. i discovered this while trying to get results for bug with authenticated scan. in my request file, i use post http method. but looking into the generated traffic file, i can see that commix uses get method except for the first request where it uses post method correctly. running latest commix from master. \n",
      "\n",
      "Original Title:\n",
      " commix sends requests with get method even if post method used in request file\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " first from flow in uk south\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 143 =================\n",
      "\n",
      "\"https://github.com/koorellasuresh/UKRegionTest/issues/9086\"\n",
      "Issue Body:\n",
      " first from flow in uk south \n",
      "\n",
      "Original Title:\n",
      " first from flow in uk south\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " first from flow in uk south\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 78 =================\n",
      "\n",
      "\"https://github.com/phonegap/phonegap-plugin-media-stream/issues/21\"\n",
      "Issue Body:\n",
      " expected behaviour only set video actual behaviour error in success callbackid: mediastreams328524481 : typeerror: mediadevice.getusermedia is not a function reproduce scenario including but not limited to add plugin and call funcion in read steps to reproduce add function : function face { navigator.mediadevices.getusermedia { 'video': { facingmode: 'user' } } .then function mediastream { // do something with the media stream } ; } call in ondeviceready: function { this.receivedevent 'deviceready' ; face }, platform and version eg. android 5.0 or ios 9.2.1 android 6 android what device vendor e.g. samsung, htc, sony... android moto g 4 cordova cli version and cordova platform version cordova --version e.g. 6.0.0 7.1.0 cordova platform version android e.g. 4.1.1 plugin version 6.3.0 cordova plugin version | grep phonegap-plugin-image-capture e.g. 1.5.3 phonegap-plugin-media-stream 1.2.1 mediastream sample code that illustrates the problem logs taken while reproducing problem in run android, error : 12-04 20:35:59.398 11861-11861/com.example.hello i/chromium: info:console 20 received event: deviceready , source: file:///android_asset/www/js/index.js 20 12-04 20:35:59.398 11861-11861/com.example.hello d/systemwebchromeclient: file:///android_asset/www/cordova.js: line 313 : error in success callbackid: mediastreams34693017 : typeerror: mediadevice.getusermedia is not a function \n",
      "\n",
      "Original Title:\n",
      " error in success callbackid: mediastreams328524481 : typeerror: mediadevice.getusermedia is not a function\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " first from flow in uk south\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 151 =================\n",
      "\n",
      "\"https://github.com/rdlabo/ionic-wpcom/issues/3\"\n",
      "Issue Body:\n",
      " hello it is possible to add custom post or page link in menu? thanks \n",
      "\n",
      "Original Title:\n",
      " custom post or page link in menu\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " add not working\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 173 =================\n",
      "\n",
      "\"https://github.com/craftercms/craftercms/issues/632\"\n",
      "Issue Body:\n",
      " the application is not updating the counter of comments of the “view and comment” icon when the user delete a comment - user need to refresh the page. preconditions: ==================== reproducibility level: ==================== always steps: ==================== steps 1. log into the application with valid credentials 2. go to a page with social content enabled. 3. add a comment. 4.verify the counter of the view and comment icon increase in 1. 5.delete the previous created comment. current result: ==================== the counter of the view and comment icon does not decrease in 1. expected result: ==================== the counter of the view and comment icon should decrease in 1.only if the user reload the page or click in the refresh button. \n",
      "\n",
      "Original Title:\n",
      " social-ui the application is not updating the counter of comments of the “view and comment” icon when the user delete a comment - user need to refresh the thread\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " add not working\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 188 =================\n",
      "\n",
      "\"https://github.com/shuhongwu/hockeyapp/issues/12257\"\n",
      "Issue Body:\n",
      " version: 6.12.2 2650 | com.sina.weibo stacktrace <pre>articleimagecell;_renderdrawingcontext:timelinepicture:imagesize:altcontent:withcontentwidth:;articleimagecell.m;410\n",
      "articleimagecell;_renderdrawingcontext:timelinepicture:imagesize:altcontent:withcontentwidth:;articleimagecell.m;442\n",
      "articleimagecell;renderdrawingcontext:withcontentwidth:;articleimagecell.m;340\n",
      "recyclablescrollviewcell;validdrawingcontextofpragraphmodel:withcontentwidth:;recyclablescrollviewcell.m;65\n",
      "wbarticlepragraphmodel ui ;preparedrawingcontextwithuserinfo:;wbarticlepragraphmodel+ui.m;79\n",
      "wbuniversalarticle prepragraphhtmlstring ;_preparedrawingcontextbypragraphmodel:;wbuniversalarticle+prepragraphhtmlstring.m;62\n",
      "wbuniversalarticle prepragraphhtmlstring ;preparepragraphbuilderwithuserinfo:pragraphedcompleteblock:;wbuniversalarticle+prepragraphhtmlstring.m;40\n",
      "articlepragraphedhtmlstringbuilder;generatepragraphmodelarraywithtextcontent:everyblock:;articlepragraphedhtmlstringbuilder.m;73\n",
      "articlepragraphedhtmlstringbuilder;_buildpragraphmodelsarraywithhtmlstring:pragraphstartindex:every:;articlepragraphedhtmlstringbuilder.m;259\n",
      "articlepragraphedhtmlstringbuilder;generatepragraphmodelarraywithtextcontent:everyblock:;articlepragraphedhtmlstringbuilder.m;70\n",
      "wbuniversalarticle prepragraphhtmlstring ;preparepragraphbuilderwithuserinfo:pragraphedcompleteblock:;wbuniversalarticle+prepragraphhtmlstring.m;39</pre> reason objc_msgsend selector name: subtype link to hockeyapp https://rink.hockeyapp.net/manage/apps/411124/crash_reasons/152465430 https://rink.hockeyapp.net/manage/apps/411124/crash_reasons/152465430 \n",
      "\n",
      "Original Title:\n",
      " fix crash in + articleimagecell _renderdrawingcontext:timelinepicture:imagesize:altcontent:withcontentwidth: , line 410\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " make fix crash in\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 116 =================\n",
      "\n",
      "\"https://github.com/minhaContaLtda/web/issues/2\"\n",
      "Issue Body:\n",
      " as notificações devem conter lembretes para envio das leitura dos relógios. \n",
      "\n",
      "Original Title:\n",
      " envio de notificações\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " make add not working\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 97 =================\n",
      "\n",
      "\"https://github.com/mozilla-services/syncserver/issues/92\"\n",
      "Issue Body:\n",
      " just like : https://accounts.firefox.com/ver.json cc @jbuck @jrgm \n",
      "\n",
      "Original Title:\n",
      " provide a /__version__ endpoint and return the current running version\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " make add not working\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 77 =================\n",
      "\n",
      "\"https://github.com/eddyerburgh/avoriaz/issues/61\"\n",
      "Issue Body:\n",
      " is this an expected behaviour? in that case? how can i stub child component without loosing the execution of created hook? thank you \n",
      "\n",
      "Original Title:\n",
      " shallow does not reach created hook\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " add not working\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 178 =================\n",
      "\n",
      "\"https://github.com/dparlevliet/node.bittrex.api/issues/24\"\n",
      "Issue Body:\n",
      " hello. sorry if this is out of place. i tried emailing n0mad01 and he suggested i ask here. i am trying to get the websocket api working in python as that is the language i'm working with. mostly, i want to get a live orderbook feed working. i am using 'signalr-client' https://pypi.python.org/pypi/signalr-client/0.0.7 . i have looked at the bittrex docx but i can't get it working. from requests import session from signalr import connection import time markets = 'btc-dgb', 'btc-strat' with session as session: connection = connection 'http://socket.bittrex.com/signalr', session corehub = connection.register_hub 'corehub' connection.start create new chat message handler def print_received_message data : print data if data 'nounce' is not none: print 'nounce', data 'nounce' if data 'deltas' is not none: for value in data 'deltas' : print value create error handler def print_error error : print 'error: ', error debug information, show all data def print_raw_data args, kwargs : print args, kwargs connection.received += print_raw_data with connection: corehub.server.invoke 'subscribetoexchangedeltas', 'btc-eth' corehub.client.on 'updatesummarystate', print_received_message connection.error += print_error connection.wait 1 while true: pass \n",
      "\n",
      "Original Title:\n",
      " making bittrex websocket api work in python\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " first from flow in uk south\n",
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 157 =================\n",
      "\n",
      "\"https://github.com/ianjwhitmore/ecotrust-northwest-community-forests/issues/10\"\n",
      "Issue Body:\n",
      " i know we were looking at this during our meeting, but i would still love to see these in the grid format. there is just so much white space in that list format. \n",
      "\n",
      "Original Title:\n",
      " forest stories grid rather than list\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " add add not working\n"
     ]
    }
   ],
   "source": [
    "# this method displays the predictions on random rows of the holdout set\n",
    "seq2seq_inf.demo_model_predictions(n=50, issue_df=testdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Evaluate Model: BLEU Score\n",
    "\n",
    "For machine-translation tasks such as this one, it is common to measure the accuracy of results using the [BLEU Score](https://en.wikipedia.org/wiki/BLEU).  The convenience function illustrated below uses [NLTK's corpus_bleu](https://www.nltk.org/api/nltk.translate.html#nltk.translate.bleu_score.corpus_bleu).  The output of the below convenience function is an Average of BlEU-1, BLEU-2, BLEU-3 and BLEU-4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Generating predictions.\n",
      "WARNING:root:Calculating BLEU.\n"
     ]
    }
   ],
   "source": [
    "#convenience function that generates predictions on holdout set and calculates BLEU Score\n",
    "\n",
    "bleu_score = seq2seq_inf.evaluate_model(holdout_bodies=testdf.body.tolist(), \n",
    "                                        holdout_titles=testdf.issue_title.tolist(), \n",
    "                                        max_len_title=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score (avg of BLUE 1-4) on Holdout Set: 3.053487322442532\n"
     ]
    }
   ],
   "source": [
    "print(f'BLEU Score (avg of BLUE 1-4) on Holdout Set: {bleu_score * 100}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "nav_menu": {
    "height": "263px",
    "width": "352px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
